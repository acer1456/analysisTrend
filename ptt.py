def ptt_run():

  # -*- coding: utf-8 -*-
  """Ptt專案.ipynb

  Automatically generated by Colaboratory.

  Original file is located at
      https://colab.research.google.com/drive/1LXGRhcwkiMpZjiQrbwXl8jUKJ8ySGS6s

  # 1. 安裝所需套件
  """

  # !pip install user_agent
  # !pip install monpa
  # !pip install flask-ngrok

  """## 1.1下載所需資料"""

  # !gdown --id '1yaBLpAqe2t0SmtByzqLkbg-8veb_Fb8m' --output stopwords.txt
  # !gdown --id '1PEpMs7rRzmSygFfaX0i4cAmepDoCBjGq' --output dcard.pkl
  # !gdown --id '18Jfbpqw2zGGWbJQQZN5a0aOUwI4--HoH' --output dcard_cut.pkl

  # print('START PTT')
  pathaa='./data/'
  pathweb='./web/PTT/'

  stop_words=[]
  f = open(pathaa+'stopwords.txt', 'r')
  for i in f:
    stop_words.append(i.strip())

  """# 2. 引入所需程式"""

  import requests
  import re
  import os
  import urllib
  from bs4 import BeautifulSoup
  from user_agent import generate_user_agent
  import pandas as pd
  import datetime
  import time
  import monpa
  import json
  import re
  from collections import Counter
  import numpy
  import numpy as np
  import matplotlib.pyplot as plt
  import io
  from IPython.display import display
  import plotly.express as px
  import plotly.graph_objects as go
  import pytz

  """# 3. 爬取PTT（預設抓兩個星期資料）"""

  #some method (no need to change)
  def checkformat(soup, class_tag, data, index, link):
      try:
          content = soup.select(class_tag)[index].text
          return content
      except Exception as e:
          # print('格式跑掉爬不到')
          # print 'checkformat:',str(e)
          return False
      
  def cralwer(url):
      r = requests.get(url, headers={ 'user-agent': generate_user_agent() })
      soup = BeautifulSoup(r.text,"html.parser")
      return soup

  def timeCom_o(x):
    x = datetime.datetime.strptime(x, "%Y-%m-%d")
    return x

  def timeCom(x):
    x = str(datetime.datetime.now(pytz.timezone('Asia/Taipei')).year)+'/'+x
    x = datetime.datetime.strptime(x, "%Y/%m/%d")
    return x

  def getforum(fourm):
    global df1
    nextpage="https://www.ptt.cc/bbs/"+fourm
    dateright=True
    while dateright == True:
      soup = cralwer(nextpage)
      rent = soup.find_all('div', class_='r-ent')
      for i in range(len(rent)):
        try:
          title = rent[i].find('a').text.strip()
        except:
          continue
        if title[0:4] not in ['[公告]','[閒聊]']:
          timee = rent[i].find('div',class_='date').text.strip()
          # if timeCom(days[1]) >= timeCom(timee) >= timeCom(days[-1]):
          if (timeCom_o(today) > timeCom(timee) >= timeCom_o(endday)) and timee not in oldlist:
            path = 'https://www.ptt.cc' + rent[i].find('a')['href']
            article = cralwer(path)
            time.sleep(0.3)
            try:
              content = article.find(id="main-content").text
            except:
              continue
            target_content = u'※ 發信站: 批踢踢實業坊(ptt.cc),'
            content = content.split(target_content)
            check = checkformat(article, '.article-meta-value', 'date', 3, path)
            if check:
              content = content[0].split(check)
              main_content = content[1].replace('\n', '  ')
            else:
              main_content = article.find(id="main-content").text
            comment = article.find_all('div', class_='push')
            comments=[]
            for j in range(len(comment)):
              comments.append({'tag':comment[j].find('span',class_='push-tag').text.strip(),
                          'userid':comment[j].find('span',class_='push-userid').text.strip(),
                          'content':comment[j].find('span',class_='push-content').text.strip(),
                          'time':comment[j].find('span',class_='push-ipdatetime').text.strip()})
            articles = {'nrec': rent[i].find('div', class_='nrec').text.strip(),
                            'author': rent[i].find('div',class_='author').text.strip(),
                            'time': rent[i].find('div',class_='date').text.strip(),
                            'title': rent[i].find('a').text.strip(),
                            'content':main_content.strip(),
                            'comments' : comments,
                            'fourm' : fourm,
                            'link': 'https://www.ptt.cc'+rent[i].find('a')['href']}
            # print(rent[i].find('div',class_='date').text.strip(), rent[i].find('a').text.strip())
            df1 = df1.append(articles, ignore_index=True)
            # print(timee, 'Done')
          elif timeCom_o(today) == timeCom(timee):
            # print(timee, 'Today No')
            pass
          elif timeCom(timee) < timeCom_o(endday):
            # print(timee, 'other NO')
            dateright=False

      #換頁
      btn = soup.select('div.btn-group-paging > a')
      nextpage = "https://www.ptt.cc/"+btn[1]['href']
      # print(timee, nextpage)

  Board_Name = 'fund'

  days=15
  today = datetime.datetime.now(pytz.timezone('Asia/Taipei')).strftime("%Y-%m-%d")
  endday = (datetime.datetime.now(pytz.timezone('Asia/Taipei')) - datetime.timedelta(days=days)).strftime("%Y-%m-%d")

  if os.path.isfile(pathaa+'ptt.pkl'):
    df_oldlist = pd.read_pickle(pathaa+'ptt.pkl')
    oldlist = list(set(df_oldlist.time))
    if timeCom(sorted(list(set(df_oldlist.time)))[-1]) == timeCom_o(today)-datetime.timedelta(days=1):
      df1 = df_oldlist
      df_new = df1
      a_comments=0
      for i in range(len(df1)):
        for j in range(len(df1['comments'][i])):
          if df1['comments'][i][j] != '':
            a_comments+=1
      usedays=[]
      for i in list(set(df1.time)):
        if timeCom(i) >= timeCom_o(endday):
          usedays.append(i)
      usedays = sorted(usedays)
    else:
      df1=pd.DataFrame()
      getforum(Board_Name)
      df_new = pd.concat([df_oldlist,df1], join='inner').reset_index(drop=True)
      df_new.to_pickle(pathaa+'ptt.pkl')
      usedays=[]
      for i in list(set(df_new.time)):
        if timeCom(i) >= timeCom_o(endday):
          usedays.append(i)
      usedays = sorted(usedays)
      # df1 = df1[df1['time'].isin(usedays)]
  else:
    oldlist=[]
    df1=pd.DataFrame()
    getforum(Board_Name)
    df_new = df1
    usedays=[]
    for i in list(set(df_new.time)):
      if timeCom(i) >= timeCom_o(endday):
        usedays.append(i)
    usedays = sorted(usedays)
    df1.to_pickle(pathaa+'ptt.pkl')

  """# 4. 資料清理與斷詞（自動化將清理過的不在清理）

  """

  # 呼叫自行定義的詞典(增加詞彙) 
  # monpa.load_userdict(path + "corpus/userdict_monpa.txt")

  def LongCut(long_sentence, split_char):
      seg = []
      for item in long_sentence.split(split_char):
          if item != "\n": seg.extend(monpa.cut(str(item+split_char)))
      return seg[:-1]

  def LongPseg(long_sentence, split_char):
      seg = []
      for item in long_sentence.split(split_char):
          if item != "\n": seg.extend(monpa.pseg(item+split_char))
      return seg[:-1]

  def cut(content):
      content=re.sub("台","臺",content)
      content=re.sub("「","",content)
      content=re.sub("」","",content)
      content=re.sub("\ufeff","",content)
      content=re.sub("\u3000","",content)
      content=re.sub('成功|賺錢|(作者|看板|標題|完整標題|完整新聞標題|發稿單位|新聞來源|發稿時間|更新時間|撰 稿 者|撰稿者|轉錄內容|完整新聞內容|附註、心得、想法|後註|原文連結)[：:]?(<[\.\w\n\s="\/_:\?\-&;%]+>)*.*(<[\.\w\n\s="\/_:\?\-&;%]+>)*\n+|(原文連結|轉錄網址|新聞網址|轉錄來源|直播連結)[：:]?|引述《|》之銘言|（.*(記者|更新|報導).*）|出席名單|重點新聞|編輯|譯者|核稿|日電|翻攝|資料照|綜合報導|華視|公視|客家電視台|中央社|青年日報|原住民族電視台|中央廣播電臺|中央廣播電台|○本板許可「台灣媒體」新聞|○外國媒體，詳見板規|\[日期/政府機關名\]|※|─|｜|\(|\)|\u3000|\n+|\d+\.|(\d+\-)+\d+|(\d+/)+\d+|\d+:\d+|<[\.\w\n\s="/_:\?\-&;%]+>|http[\.\w\n\s\?\/\-="_:&;%]+','',content)
      content=re.sub(r'[a-z]*[:.]+\S+', '', content).replace('\n','').replace('\s+', '').replace('：+', '').replace('^：+', '')
      content = LongPseg(content,"，")
      seg=[]
      segg=[]
      for w in range(len(content)):
        if content[w][0].strip() not in stop_words:
          seg.append(content[w][0])
          segg.append(content[w][1])
      return seg, segg
      
  def pcut(x):
      x['content_token']=''
      x['content_pgtoken']=''
      for i in range(len(x)):
          content = x['content'][i]
          seg, segg = cut(content)
          x['content_token'][i]=seg
          x['content_pgtoken'][i]=segg
          # 每處理完10篇文章，報告進度。******    
          # if i%100==0:
          #     print (i, 'done')


  def combine_comment_content(df1):
    cut_content = df1[['content','time']]
    df_ccomments=pd.DataFrame()
    a_comments=0
    for i in range(len(df1)):
      for j in range(len(df1['comments'][i])):
        content = df1['comments'][i][j]['content'].replace(': ','')
        if content != '':
          a_comments+=1
          b={'content': content, 'time': df1['time'][i]}
          df_ccomments = df_ccomments.append(b, ignore_index=True)
    df_cut = pd.concat([df_ccomments,cut_content], join='inner').reset_index(drop=True)
    return df_cut, a_comments

  def delpsegg(x):
    del_pseg=["Nh","Ncd","Nep","Neu","Nf","Nes","Nega","Ng","VK","VE","VJ","VL","D","Dfa","Da","P","FW","C","Cbb","Caa",
              "SHI","PERIODCATEGORY","VC","Nd","DE"]
    new_token=[]
    new_pseg=[]
    # 每篇文章依序進行
    for i in range(len(x)):
        tem_token=[]
        tem_pseg=[]
        # 文章的字詞，逐一檢查
        for j in range(len(x.content_token[i])):
            # 逗點符號保留
            if len(x.content_token[i][j])>1: #or df.content_token[i][j] in ["，","。","：","？"]:
                # 去除不要的詞性，空白開頭的也刪除
                if x.content_pgtoken[i][j] not in del_pseg and x.content_token[i][j].startswith(" ")==False : 
                    # 異體字替代
                    # 含有除特殊字串者去除
    #                 deletewords=['[',']',"'",'\ufeff']
                    yes=x.content_token[i][j].replace("台","臺")
                    #合乎條件的字詞                
                    tem_token.append(yes)
                    # 合乎條件的詞性
                    tem_pseg.append(x.content_pgtoken[i][j])                
        # 每篇累計            
        new_token.append(tem_token)     
        new_pseg.append(tem_pseg)
    # DataFrame 的欄位替代成新的 list
    x["token"]=new_token  
    x["pseg"]=new_pseg
    # print('del psegg Done')

  if os.path.isfile(pathaa+'ptt_cut.pkl'):
    if timeCom(sorted(list(set(df_oldlist.time)))[-1]) == timeCom_o(today)-datetime.timedelta(days=1):
      df = pd.read_pickle(pathaa+'ptt_cut.pkl')
    else:
      df_cut_old = pd.read_pickle('pathaa+ptt_cut.pkl')
      df_cut, a_comments = combine_comment_content(df1)
      pcut(df_cut)
      delpsegg(df_cut)
      df = pd.concat([df_cut_old,df_cut], join='inner').reset_index(drop=True)
      df.to_pickle(pathaa+'ptt_cut.pkl')
      df = df[df['time'].isin(usedays)]
  else:
    df_cut, a_comments = combine_comment_content(df1)
    pcut(df_cut)
    delpsegg(df_cut)
    df_cut.to_pickle(pathaa+'ptt_cut.pkl')
    df = pd.read_pickle(pathaa+'ptt_cut.pkl')

  """# 5. 分析


  """

  import os
  import sys
  import numpy
  from sklearn.svm import LinearSVC
  from sklearn.feature_extraction.text import CountVectorizer
  from sklearn.feature_extraction.text import TfidfTransformer
  from sklearn.feature_extraction.text import TfidfVectorizer
  corpus=[]
  aaa = df['token']
  for i in aaa:
      corpus.append(" ".join(i))
  vectorizer = TfidfVectorizer(min_df=1, stop_words=stop_words, max_features=10)
  vectorizer.fit_transform(corpus)
  lista = vectorizer.get_feature_names()
  aa =[]
  # print(lista)
  tfidf = vectorizer.fit_transform(corpus)
  df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names())
  df_tfidf = df_tfidf.append(pd.Series(), ignore_index=True)
  keyword={}
  for i in lista:
      df_tfidf[i][len(corpus)] = df_tfidf[i].sum()
      keyword.update({i: 0})
  for i in range(len(df)):
    for j in range(len(lista)):
      if df['content'][i].find(lista[j])>-1:
        keyword[lista[j]] += 1 
  # df_tfidf.loc[len(df_tfidf)-1]

  # tfidf關鍵字 及 文件次數
  total=0
  df_tfidf_word=pd.DataFrame()
  for i in df_tfidf.loc[len(df_tfidf)-1]:
      total +=i
  for i in range(len(df_tfidf.loc[len(df_tfidf)-1])):
      a={'word':df_tfidf.columns[i], 'hot':df_tfidf.loc[len(df_tfidf)-1][i]/total*100}
      df_tfidf_word = df_tfidf_word.append(a, ignore_index=True)


  for i in range(len(df_tfidf_word)):
      for j in sorted(keyword.items(), key=lambda x:x[1], reverse=True):
          if df_tfidf_word['word'][i] == j[0]:
              df_tfidf_word['word'][i]=df_tfidf_word['word'][i]+' '+str(j[1])
              
  # display(df_tfidf_word)

  fig1 = px.pie(df_tfidf_word, values='hot', names='word', title='TFIDF關鍵字 出現文件數',width=500, height=500)
  fig1.update_traces(textposition='inside', textinfo='percent+label')
  # fig1.show()

  # 讀取斷好詞的各篇文章
  seg_list=aaa
  tokens=[]
  for n in seg_list:
      tokens=tokens+n
  word_count=Counter(tokens)
  # 不重複的字詞/總共字詞
  wordd=str(len(set(tokens)))+'/'+str(len(tokens))

  #計算各字詞的詞頻，列出前 10個
  # print ('計算各字詞的詞頻，列出前 10個')

  df_tf_word=pd.DataFrame()
  for w,c in word_count.most_common(10):   # 前10大
      a={'詞彙':w, '詞頻':c}
      df_tf_word = df_tf_word.append(a, ignore_index=True)
      
  fig2 = px.pie(df_tf_word, values='詞頻', names='詞彙', title='各字詞詞頻',width=500, height=500)
  fig2.update_traces(textposition='inside', textinfo='percent+label')
  # fig2.show()

  # 印出最常出現的配對  word_pair1_count
  word_pair1_count=Counter()
  for i in range(len(tokens)-1):
      # 字詞和下一個字詞，構成一個 tuple
      (w1,w2)=tokens[i],tokens[i+1]
      if w1 != w2:
        # 累計同樣的配對的次數
        word_pair1_count[(w1,w2)]+=1

  pairword=[]
  pairnum=[]
  for w,c in word_pair1_count.most_common(15):   # 前10大
  #     print(w,c)
      pairword.append(w[0]+'-'+w[1])
      pairnum.append(c)
      
  import plotly.graph_objects as go
  import plotly.io as pio
  fig = go.Figure([go.Bar(x=pairword, y=pairnum)])
  fig.update_layout(title='所有字詞配對', width=700, height=450)
  # fig.show()

  # 配對有包含tfidf關鍵字(距離五個內) word_pair3_count
  n=5
  word_pair3_count=Counter()   # 總對數
  for a in range(len(seg_list)):
    for i in range(len(seg_list[a])-1):
      for j in range(1,n+1):      # n 為 window size, j 為 distance
        if i+j < len(seg_list[a]):
          (w1,w2)=seg_list[a][i],seg_list[a][i+j]                
          # 累計配對數目
          if w1 != w2:
            word_pair3_count[(w1,w2)]+=1

  kw=lista
  tfidf5pairword=[]
  tfidf5pairnum=[]
  for (w1,w2),c in word_pair3_count.most_common(20):   # 前20大
      if w1 in kw or w2 in kw:   # 包含某個關鍵字   
          tfidf5pairword.append(w1+'-'+w2)
          tfidf5pairnum.append(c)

  import plotly.graph_objects as go
  import plotly.io as pio
  fig3 = go.Figure([go.Bar(x=tfidf5pairword, y=tfidf5pairnum)])
  fig3.update_layout(title='tfidf關鍵字與其他關鍵字 (距離五個內)', width=700, height=450)
  # fig3.show()

  #tfidf配對tfidf關鍵字 平均距離 及 對數 word_pair4_count
  word_pair4_count=Counter()      # 總對數
  word_pair_distance_count=Counter()   
  for a in range(len(seg_list)):      # 對數 by distance
    for i in range(len(seg_list[a])-1):
      for j in range(1,len(seg_list[a])):      # n 為 window size, j 為 distance
        if i+j < len(seg_list[a]):
          (w1,w2)=seg_list[a][i],seg_list[a][i+j]
          # if len(w1)>1 and len(w2)>1:
          # 累計配對數目
          if w1 != w2:
            word_pair4_count[(w1,w2)]+=1
            # 依不同的距離累計
            word_pair_distance_count[(w1,w2,j)]+=1

  mean_distance_count=Counter()
  # 所有的配對，距離與對數
  for (w1,w2, distance),c in word_pair_distance_count.most_common():
  # 平均距離 = 距離 * 對數 / 總次數
    mean_distance_count[(w1,w2)]+=distance*c/word_pair4_count[(w1,w2)]

  #tfidf配對tfidf關鍵字 平均距離 及 對數'
  df_tfidf_meanDistance=pd.DataFrame()
  for (w1,w2),mean_distance in sorted(mean_distance_count.most_common(), key=lambda i: i[1], reverse=False):
    for a in range(len(lista)-1):
      for b in range(len(lista)):
        if lista[a] != lista[b]:
          if (w1 in lista[a] and w2 in lista[b]):
            cc={'對數':str(word_pair4_count[(w1,w2)]),'平均距離': str(round(mean_distance)),'關鍵字2':w2, '關鍵字1':w1}
            df_tfidf_meanDistance = df_tfidf_meanDistance.append(cc,ignore_index=True)

  headerColor = 'mediumseagreen'
  rowEvenColor = 'lightcyan'
  rowOddColor = 'white'

  fig4 = go.Figure(data=[go.Table(
    header=dict(
      values=['關鍵字1','關鍵字2','平均距離','對數'],
      line_color='darkslategray',
      fill_color=headerColor,
      align=['center'],
      font=dict(color='white', size=15)
    ),
    cells=dict(
      values=[df_tfidf_meanDistance.關鍵字1, df_tfidf_meanDistance.關鍵字2, df_tfidf_meanDistance.平均距離, df_tfidf_meanDistance.對數],
      line_color='darkslategray',
      # 2-D list of colors for alternating rows
      fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*5],
      align = ['center'],
      font = dict(color = 'darkslategray', size =14),
      height=27
      ))
  ])
  fig4.update_layout(title='TFIDF配對TFIDF',width=500, height=500)
  # fig4.show()

  mostlikepost=[]
  df2=pd.DataFrame()
  for i in range(len(df_new)):
      try:
          nrec = int(df_new['nrec'][i])
          if nrec>=20:
              mostlikepost.append(i)
      except:
          pass
  for i in mostlikepost:
  #     print(dict(df1.loc[i]))
      df2 = df2.append(dict(df_new.loc[i]),ignore_index=True)
  for i in range(len(df2)):
      df2['title'][i] = '<a href="'+df2['link'][i]+'">'+df2['title'][i]+'</a>' +'  '+df2['time'][i]+'  '+df2['nrec'][i]
  df2 = df2[['title']]

  headerColor = 'mediumseagreen'
  rowEvenColor = 'lightcyan'
  rowOddColor = 'white'

  fig5 = go.Figure(data=[go.Table(
    header=dict(
      values=['標題/時間/討論度'],
      line_color='darkslategray',
      fill_color=headerColor,
      align=['center'],
      font=dict(color='white', size=15)
    ),
    cells=dict(
      values=[df2.title],
      line_color='darkslategray',
      fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor]*len(df2)],
      align = ['center'],
      font = dict(color = 'darkslategray', size =14),
      height=25,
      ))
  ])
  fig5.update_layout(title='熱門文章(討論度>=20)',width=600, height=300)
  # fig5.show()

  """# 6. 輸出網頁視覺"""

  fig=fig.to_html(full_html=False, include_plotlyjs='cdn')
  fig1=fig1.to_html(full_html=False, include_plotlyjs='cdn')
  fig2=fig2.to_html(full_html=False, include_plotlyjs='cdn')
  fig3=fig3.to_html(full_html=False, include_plotlyjs='cdn')
  fig4=fig4.to_html(full_html=False, include_plotlyjs='cdn')
  fig5=fig5.to_html(full_html=False, include_plotlyjs='cdn')
  # tabble = df2.to_html(classes='table')

  a='''
  <html>
  <head>
  <meta charset=\"utf-8\">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>
  <style>
  .bg-purple { background-color: #6f42c1; }
  .center { text-align:center; }
  </style>
  </head>
  <body>
  <div class="container">

  <div class="bg-light p-5 rounded">
  <h1>PTT 趨勢分析 <span style="font-size:20px">'''+usedays[0]+'至'+usedays[-1]+'''</span></h1>

  <div class="row">
  <div class="col-lg-4">
      <div class=" p-3 my-3 text-white bg-primary rounded shadow-sm">
          <div class="lh-1 center">
            <h2 class="h6 mb-0 text-white lh-1">文章數量</h2>
            <h1>'''+str(len(df_new))+'''</h1>
          </div>
      </div>
  </div>
  <div class="col-lg-4">
      <div class="p-3 my-3 text-white bg-success rounded shadow-sm">
          <div class="lh-1 center">
            <h2 class="h6 mb-0 text-white lh-1">留言數量</h2>
            <h1>'''+str(a_comments)+'''</h1>
          </div>
      </div>
  </div>
  <div class="col-lg-4">
      <div class="p-3 my-3 text-white bg-danger rounded shadow-sm">
          <div class="lh-1 center">
            <h2 class="h6 mb-0 text-white lh-1">不重複的字詞/總共字詞</h2>
            <h1>'''+wordd+'''</h1>
          </div>
      </div>
  </div>
  </div>


  </div>

  <div class="row">
  <div class="col-sm-5">
  '''+fig2+'''
  </div>
  <div class="col-sm-7">
  '''+fig+'''
  </div>
  </div>

  <div class="row">
  <div class="col-sm-5">
  '''+fig1+'''
  </div>
  <div class="col-sm-7">
  '''+fig3+'''
  </div>
  </div>

  <div class="row">
  <div class="col-sm-5">
  '''+fig4+'''
  </div>

  <div class="col-sm-7">
  '''+fig5+'''
  </div>
  </div>

  </div>
  </body>
  </html>
  '''

  with open(pathweb+'ptt_'+usedays[0].replace('/','')+'至'+usedays[-1].replace('/','')+'.html', 'w',encoding='utf-8') as f:
      f.write(a)
    
  # with open(pathweb+'index.html', 'w',encoding='utf-8') as f:
  #     f.write(a)
  
  return a

  """# 7. 網頁呈現"""

  # print('DONE')

  # import webbrowser
  # from flask_ngrok import run_with_ngrok
  # from flask import Flask

  # app = Flask(__name__)
  # run_with_ngrok(app)

  # @app.route("/")
  # def home():
  #     return a
      
  # if __name__ == "__main__":
  #   webbrowser.open_new('http://127.0.0.1:5000/')
  #   app.run()