def cop_run():
    # -*- coding: utf-8 -*-
    """外商基金專案.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1POsSbxhh9X8ID0LJghXhmLyE8mRR60n8
    """

    # !pip install user_agent
    # !pip install monpa

    # print('START')
    pathaa='./data/'
    pathweb='./web/Cop/'

    import requests
    import re
    import os
    import urllib
    from bs4 import BeautifulSoup
    from user_agent import generate_user_agent
    import pandas as pd
    import datetime
    import time
    import monpa
    import json
    import re
    from collections import Counter
    import numpy
    import numpy as np
    import matplotlib.pyplot as plt
    import io
    from IPython.display import display
    import plotly.express as px
    import plotly.graph_objects as go

    # !gdown --id '1yaBLpAqe2t0SmtByzqLkbg-8veb_Fb8m' --output stopwords.txt

    stop_words=[]
    f = open(pathaa+'stopwords.txt', 'r')
    for i in f:
        stop_words.append(i.strip())

    def cralwer(url):
        r = requests.get(url, headers={ 'user-agent': generate_user_agent() })
        r.encoding='utf-8'
        soup = BeautifulSoup(r.text,"html.parser")
        return soup
    def clear(x):
        x = x.strip()
        return x

    import pytz
    today = datetime.datetime.now(pytz.timezone('Asia/Taipei')).strftime("%Y-%m-%d")
    today

    df1 = pd.DataFrame()

    #中國信託投信
    url = "https://www.ctbcinvestments.com.tw/fund/focusfund/list.html"
    soup = cralwer(url)
    articles = soup.select('.col-12.col-md-6.col-lg-4.i-item.mb-4 h5')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '中國信託投信'}
        df1 = df1.append(a, ignore_index=True)

    #凱基投信
    url = "https://www.kgifund.com.tw/focusfund/index.html"
    soup = cralwer(url)
    articles = soup.select('.fundName')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '凱基投信'}
        df1 = df1.append(a, ignore_index=True)

    #國泰投信
    url = "https://www.cathaysite.com.tw/"
    soup = cralwer(url)
    articles = soup.select('.w3-third.w3-container span')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '國泰投信'}
        df1 = df1.append(a, ignore_index=True)

    #安聯投信
    url = "https://tw.allianzgi.com/zh-tw/home/announcement/latest-announcement/2021-03-31-1"
    soup = cralwer(url)
    articles = soup.select('.body-one a')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '安聯投信'}
        df1 = df1.append(a, ignore_index=True)

    #富邦投信
    url = "https://www.fubon.com/asset-management/fund/info/by_nav"
    soup = cralwer(url)
    articles = soup.select('.title a')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '富邦投信'}
        df1 = df1.append(a, ignore_index=True)

    #復華投信
    url = "https://www.fhtrust.com.tw/funds/fund_intro_TIF.aspx"
    soup = cralwer(url)
    articles = soup.select('td a')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '復華投信'}
        df1 = df1.append(a, ignore_index=True)

    #群益投信
    url = "https://www.capitalfund.com.tw/ETF_Area/Product"
    soup = cralwer(url)
    articles = soup.select('.list-title-name')
    for each_title in articles:
        a = {'content': each_title.text, 'source': '群益投信'}
        df1 = df1.append(a, ignore_index=True)
    df1

    df1.to_pickle(pathaa+'/Cop/cofund_'+today+'.pkl')
    df = df1

    def LongCut(long_sentence, split_char):
        seg = []
        for item in long_sentence.split(split_char):
            if item != "\n": seg.extend(monpa.cut(str(item+split_char)))
        return seg[:-1]

    def LongPseg(long_sentence, split_char):
        seg = []
        for item in long_sentence.split(split_char):
            if item != "\n": seg.extend(monpa.pseg(item+split_char))
        return seg[:-1]

    def cut(content):
        content=re.sub("台","臺",content)
        content=re.sub("「","",content)
        content=re.sub("」","",content)
        content=re.sub("\ufeff","",content)
        content=re.sub("\u3000","",content)
        content=re.sub('(作者|看板|標題|完整標題|完整新聞標題|發稿單位|新聞來源|發稿時間|更新時間|撰 稿 者|撰稿者|轉錄內容|完整新聞內容|附註、心得、想法|後註|原文連結)[：:]?(<[\.\w\n\s="\/_:\?\-&;%]+>)*.*(<[\.\w\n\s="\/_:\?\-&;%]+>)*\n+|(原文連結|轉錄網址|新聞網址|轉錄來源|直播連結)[：:]?|引述《|》之銘言|（.*(記者|更新|報導).*）|出席名單|重點新聞|編輯|譯者|核稿|日電|翻攝|資料照|綜合報導|華視|公視|客家電視台|中央社|青年日報|原住民族電視台|中央廣播電臺|中央廣播電台|○本板許可「台灣媒體」新聞|○外國媒體，詳見板規|\[日期/政府機關名\]|※|─|｜|\(|\)|\u3000|\n+|\d+\.|(\d+\-)+\d+|(\d+/)+\d+|\d+:\d+|<[\.\w\n\s="/_:\?\-&;%]+>|http[\.\w\n\s\?\/\-="_:&;%]+','',content)
        content=re.sub(r'[a-z]*[:.]+\S+', '', content).replace('\n','').replace('\s+', '').replace('：+', '').replace('^：+', '')
        content = LongPseg(content,"，")
        seg=[]
        segg=[]
        for w in range(len(content)):
            if content[w][0] not in stop_words:
                seg.append(content[w][0])
                segg.append(content[w][1])
        return seg, segg
        
    def pcut(df, token, pgtoken):
        for i in range(len(df)):
            content = df[i]
            seg, segg = cut(content)
            token[i]=seg
            pgtoken[i]=segg
            # # 每處理完10篇文章，報告進度。******    
            # if i%10==0:
            #     print (i, 'done')

    # 呼叫自行定義的詞典(增加詞彙) 
    # monpa.load_userdict(path + "corpus/userdict_monpa.txt")
    df['content_token']=''
    df['content_pgtoken']=''

    # pcut(df['title'], df['title_token'], df['title_pgtoken'])
    pcut(df['content'], df['content_token'], df['content_pgtoken'])

    # 設定條件：
    # 逗點符號保留，兩個字以上的字詞才保留，
    # 去除不要的詞性
    del_pseg=["Nh","Ncd","Nep","Neu","Nf","Nes","Nega","Ng","VK","VE","VJ","VL","D","Dfa","Da","P","FW","C","Cbb","Caa",
            "SHI","PERIODCATEGORY","VC","Nd","DE"]

    new_token=[]
    new_pseg=[]
    # 每篇文章依序進行
    for i in range(len(df)):
        tem_token=[]
        tem_pseg=[]
        # 文章的字詞，逐一檢查
        for j in range(len(df.content_token[i])):
            # 逗點符號保留
            if len(df.content_token[i][j])>1: #or df.content_token[i][j] in ["，","。","：","？"]:
                # 去除不要的詞性，空白開頭的也刪除
                if df.content_pgtoken[i][j] not in del_pseg and df.content_token[i][j].startswith(" ")==False : 
                    # 異體字替代
                    # 含有除特殊字串者去除
                    yes=df.content_token[i][j].replace("台","臺")
                    #合乎條件的字詞                
                    tem_token.append(yes)
                    # 合乎條件的詞性
                    tem_pseg.append(df.content_pgtoken[i][j])                
        # 每篇累計            
        new_token.append(tem_token)     
        new_pseg.append(tem_pseg)
        
    # DataFrame 的欄位替代成新的 list
    df["token"]=new_token  
    df["pseg"]=new_pseg

    import os
    import sys
    import numpy
    from sklearn.svm import LinearSVC
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.feature_extraction.text import TfidfVectorizer

    corpus=[]
    aaa = df['token']
    for i in aaa:
        corpus.append(" ".join(i))
        
    vectorizer = TfidfVectorizer(min_df=1, stop_words=stop_words, max_features=10)
    vectorizer.fit_transform(corpus)
    lista = vectorizer.get_feature_names()
    aa =[]

    tfidf = vectorizer.fit_transform(corpus)
    df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names())

    df_tfidf = df_tfidf.append(pd.Series(), ignore_index=True)
    keyword={}
    for i in lista:
        df_tfidf[i][len(corpus)] = df_tfidf[i].sum()
        keyword.update({i: 0})

    for i in range(len(df)):
        for j in range(len(lista)):
            if df['content'][i].find(lista[j])>-1:
                keyword[lista[j]] += 1 
    # df_tfidf.loc[len(df_tfidf)-1]

    # 讀取斷好詞的各篇文章
    seg_list=aaa
    tokens=[]
    for n in seg_list:
        tokens=tokens+n
    word_count=Counter(tokens)
    # 不重複的字詞/總共字詞
    wordd=str(len(set(tokens)))+'/'+str(len(tokens))

    # 印出最常出現的配對  word_pair1_count
    word_pair1_count=Counter()
    for i in range(len(tokens)-1):
        # 字詞和下一個字詞，構成一個 tuple
        (w1,w2)=tokens[i],tokens[i+1]
        if w1 != w2:
            # 累計同樣的配對的次數
            word_pair1_count[(w1,w2)]+=1

    # 配對關鍵字 tfidf配對tfidf距離五個字內 word_pair_count
    n=5
    word_pair_count=Counter()     # 總對數
    for a in range(len(seg_list)):      # 對數 by distance
        for i in range(len(seg_list[a])-1):
            for j in range(1,n+1):      # n 為 window size, j 為 distance
                if i+j < len(seg_list[a]):
                    (w1,w2)=seg_list[a][i],seg_list[a][i+j]
                    # if len(w1)>1 and len(w2)>1:
                    if w1 != w2:
                        # 累計配對數目
                        word_pair_count[(w1,w2)]+=1

    df_distance = pd.DataFrame('0',columns=lista, index=lista)   
    for a in range(len(lista)-1):
        for b in range(len(lista)):
            for (w1,w2),c in word_pair_count.most_common():   
                if (w1 in lista[a] and w2 in lista[b]):   
                    for x in range(len(df_distance)):
                        df_distance[lista[x]][x] = 1
                        for y in range(len(df_distance)):
                            if lista[x] == w1 and df_distance.index[y] == w2:
                                df_distance[lista[x]][y] = c
                                df_distance[lista[y]][x] = c

    # 配對有包含tfidf關鍵字(相鄰) word_pair2_count
    kw=lista
    word_pair2_count=Counter()
    for i in range(len(tokens)-1):
        (w1,w2)=tokens[i],tokens[i+1]
        if w1 != w2:
            word_pair2_count[(w1,w2)]+=1

    # 配對有包含tfidf關鍵字(距離五個內) word_pair3_count
    n=5
    word_pair3_count=Counter()   # 總對數
    for a in range(len(seg_list)):
        for i in range(len(seg_list[a])-1):
            for j in range(1,n+1):      # n 為 window size, j 為 distance
                if i+j < len(seg_list[a]):
                    (w1,w2)=seg_list[a][i],seg_list[a][i+j]                
                    # 累計配對數目
                    if w1 != w2:
                        word_pair3_count[(w1,w2)]+=1

    #tfidf配對tfidf關鍵字 平均距離 及 對數 word_pair4_count
    word_pair4_count=Counter()      # 總對數
    word_pair_distance_count=Counter()   
    for a in range(len(seg_list)):      # 對數 by distance
        for i in range(len(seg_list[a])-1):
            for j in range(1,len(seg_list[a])):      # n 為 window size, j 為 distance
                if i+j < len(seg_list[a]):
                    (w1,w2)=seg_list[a][i],seg_list[a][i+j]
                    if w1 != w2:
                        # if len(w1)>1 and len(w2)>1:
                        # 累計配對數目
                        word_pair4_count[(w1,w2)]+=1
                        # 依不同的距離累計
                        word_pair_distance_count[(w1,w2,j)]+=1

    mean_distance_count=Counter()
    # 所有的配對，距離與對數
    for (w1,w2, distance),c in word_pair_distance_count.most_common():
        # 平均距離 = 距離 * 對數 / 總次數
        mean_distance_count[(w1,w2)]+=distance*c/word_pair4_count[(w1,w2)]

    #tfidf配對tfidf關鍵字 平均距離 及 對數'
    df_tfidf_meanDistance=pd.DataFrame()
    for (w1,w2),mean_distance in sorted(mean_distance_count.most_common(), key=lambda i: i[1], reverse=False):
        for a in range(len(lista)-1):
            for b in range(len(lista)):
                if lista[a] != lista[b]:
                    if (w1 in lista[a] and w2 in lista[b]):
                        cc={'對數':str(word_pair4_count[(w1,w2)]),'平均距離': str(round(mean_distance)),'關鍵字2':w2, '關鍵字1':w1}
                        df_tfidf_meanDistance = df_tfidf_meanDistance.append(cc,ignore_index=True)

    headerColor = 'mediumseagreen'
    rowEvenColor = 'lightcyan'
    rowOddColor = 'white'

    fig4 = go.Figure(data=[go.Table(
    header=dict(
        values=['關鍵字1','關鍵字2','平均距離','對數'],
        line_color='darkslategray',
        fill_color=headerColor,
        align=['center'],
        font=dict(color='white', size=15)
    ),
    cells=dict(
        values=[df_tfidf_meanDistance.關鍵字1, df_tfidf_meanDistance.關鍵字2, df_tfidf_meanDistance.平均距離, df_tfidf_meanDistance.對數],
        line_color='darkslategray',
        # 2-D list of colors for alternating rows
        fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*5],
        align = ['center'],
        font = dict(color = 'darkslategray', size =14),
        height=27
        ))
    ])
    fig4.update_layout(title='TFIDF配對TFIDF',width=500, height=500)
    # fig4.show()

    kw=lista
    tfidf5pairword=[]
    tfidf5pairnum=[]
    for (w1,w2),c in word_pair3_count.most_common(20):   # 前20大
        if w1 in kw or w2 in kw:   # 包含某個關鍵字   
            tfidf5pairword.append(w1+'-'+w2)
            tfidf5pairnum.append(c)

    import plotly.graph_objects as go
    import plotly.io as pio
    fig3 = go.Figure([go.Bar(x=tfidf5pairword, y=tfidf5pairnum)])
    fig3.update_layout(title='tfidf關鍵字與其他關鍵字 (距離五個內)', width=700, height=450)
    # fig3.show()

    pairword=[]
    pairnum=[]
    for w,c in word_pair1_count.most_common(15):   # 前10大
    #     print(w,c)
        pairword.append(w[0]+'-'+w[1])
        pairnum.append(c)
        
    import plotly.graph_objects as go
    import plotly.io as pio
    fig = go.Figure([go.Bar(x=pairword, y=pairnum)])
    fig.update_layout(title='所有字詞配對', width=700, height=450)
    # fig.show()

    # tfidf關鍵字 及 文件次數
    total=0
    df_tfidf_word=pd.DataFrame()
    for i in df_tfidf.loc[len(df_tfidf)-1]:
        total +=i
    for i in range(len(df_tfidf.loc[len(df_tfidf)-1])):
        a={'word':df_tfidf.columns[i], 'hot':df_tfidf.loc[len(df_tfidf)-1][i]/total*100}
        df_tfidf_word = df_tfidf_word.append(a, ignore_index=True)


    for i in range(len(df_tfidf_word)):
        for j in sorted(keyword.items(), key=lambda x:x[1], reverse=True):
            if df_tfidf_word['word'][i] == j[0]:
                df_tfidf_word['word'][i]=df_tfidf_word['word'][i]+' '+str(j[1])
                
    # display(df_tfidf_word)

    fig1 = px.pie(df_tfidf_word, values='hot', names='word', title='TFIDF關鍵字 出現文件數',width=500, height=500)
    fig1.update_traces(textposition='inside', textinfo='percent+label')
    # fig1.show()

    #計算各字詞的詞頻，列出前 10個
    df_tf_word=pd.DataFrame()
    for w,c in word_count.most_common(10):   # 前10大
        a={'詞彙':w, '詞頻':c}
        df_tf_word = df_tf_word.append(a, ignore_index=True)
        
    fig2 = px.pie(df_tf_word, values='詞頻', names='詞彙', title='各字詞詞頻',width=500, height=500)
    fig2.update_traces(textposition='inside', textinfo='percent+label')
    # fig2.show()

    sourcee={}
    df_doc_num=pd.DataFrame()
    for i in list(set(df.source)):
        sourcee.update({i: 0})
    for i in range(len(list(sourcee.keys()))):
        for j in range(len(df.source)):
            if list(sourcee.keys())[i] == df.source[j]:
                sourcee[list(sourcee.keys())[i]] += 1
                # a={'公司':list(sourcee.keys())[i], '數量': 1}
        
    #計算各投信文章數量
    # print ('計算各投信文章數量')
    df_doc_num = pd.DataFrame(list(sourcee.items()),columns=['公司', '數量'])
    df_doc_num
    fig5 = px.pie(df_doc_num, values='數量', names='公司', title='各投信文章數量',width=500, height=500)
    fig5.update_traces(textposition='inside', textinfo='percent+label')
    # fig5.show()

    fig=fig.to_html(full_html=False, include_plotlyjs='cdn')
    fig1=fig1.to_html(full_html=False, include_plotlyjs='cdn')
    fig2=fig2.to_html(full_html=False, include_plotlyjs='cdn')
    fig3=fig3.to_html(full_html=False, include_plotlyjs='cdn')
    fig4=fig4.to_html(full_html=False, include_plotlyjs='cdn')
    fig5=fig5.to_html(full_html=False, include_plotlyjs='cdn')
    # tabble = df2.to_html(classes='table')

    a='''
    <html>
    <head>
    <meta charset=\"utf-8\">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>
    <style>
    .bg-purple { background-color: #6f42c1; }
    .center { text-align:center; }
    </style>
    </head>
    <body>
    <div class="container">

    <div class="bg-light p-5 rounded">
    <h1>各投信指標 分析 '''+today+''' </h1>

    <div class="row">
    <div class="col-lg-6">
        <div class=" p-3 my-3 text-white bg-primary rounded shadow-sm">
            <div class="lh-1 center">
            <h2 class="h6 mb-0 text-white lh-1">推薦總數量</h2>
            <h1>'''+str(len(df1))+'''</h1>
            </div>
        </div>
    </div>
    <div class="col-lg-6">
        <div class="p-3 my-3 text-white bg-danger rounded shadow-sm">
            <div class="lh-1 center">
            <h2 class="h6 mb-0 text-white lh-1">不重複的字詞/總共字詞</h2>
            <h1>'''+wordd+'''</h1>
            </div>
        </div>
    </div>
    </div>


    </div>

    <div class="row">
    <div class="col-sm-5">
    '''+fig2+'''
    </div>
    <div class="col-sm-7">
    '''+fig+'''
    </div>
    </div>

    <div class="row">
    <div class="col-sm-5">
    '''+fig1+'''
    </div>
    <div class="col-sm-7">
    '''+fig3+'''
    </div>
    </div>

    <div class="row">
    <div class="col-sm-5">
    '''+fig4+'''
    </div>

    <div class="col-sm-7">
    '''+fig5+'''
    </div>
    </div>

    </div>
    </body>
    </html>
    '''

    with open(pathweb+'投信_'+today+'.html', 'w',encoding='utf-8') as f:
        f.write(a)
    
    # with open(pathweb+'index.html', 'w',encoding='utf-8') as f:
    #     f.write(a)

    return a

    # import webbrowser
    # webbrowser.open('file://' + os.path.realpath(pathweb+'index.html'))


    # from flask_ngrok import run_with_ngrok
    # from flask import Flask

    # app = Flask(__name__)
    # run_with_ngrok(app)

    # @app.route("/")
    # def home():
    #     return a

    # app.run()

